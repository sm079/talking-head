{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(torch.mean(x**2, dim=-1, keepdim=True) + 1e-6)\n",
    "    \n",
    "class DepthToSpace(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(DepthToSpace, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        oh, ow = h * self.size, w * self.size\n",
    "        oc = c // (self.size * self.size)\n",
    "        x = x.view(b, self.size, self.size, oc, h, w)\n",
    "        x = x.permute(0, 3, 4, 1, 5, 2)\n",
    "        x = x.contiguous().view(b, oc, oh, ow)\n",
    "        return x\n",
    "    \n",
    "class Res(nn.Module):\n",
    "    def __init__(self, n_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(n_ch, n_ch, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(n_ch, n_ch, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.fuse = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fuse(x + self.conv(x))\n",
    "    \n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, n_ch):\n",
    "        super().__init__()\n",
    "        self.out_conv = nn.ModuleList([\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=1),\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=3, padding=1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([i(x) for i in self.out_conv], dim=1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, e_ch):\n",
    "        super().__init__()\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(     3, e_ch*1, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True), Res(e_ch*1),\n",
    "            nn.Conv2d(e_ch*1, e_ch*2, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(e_ch*2, e_ch*4, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(e_ch*4, e_ch*8, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(e_ch*8, e_ch*8, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True), Res(e_ch*8),\n",
    "            nn.Flatten(), PixelNorm()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.image_encoder(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ae_dim, d_ch, m_ch):\n",
    "        super().__init__()\n",
    "        self.image_decoder = nn.Sequential(\n",
    "            nn.Conv2d(ae_dim, d_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2), Res(d_ch*8),\n",
    "            nn.Conv2d(d_ch*8, d_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2), Res(d_ch*8),\n",
    "            nn.Conv2d(d_ch*8, d_ch*4*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2), Res(d_ch*4),\n",
    "            nn.Conv2d(d_ch*4, d_ch*2*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2), Res(d_ch*2),\n",
    "            OutConv(d_ch*2), DepthToSpace(2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.mask_decoder = nn.Sequential(\n",
    "            nn.Conv2d(ae_dim, m_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2),\n",
    "            nn.Conv2d(m_ch*8, m_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2),\n",
    "            nn.Conv2d(m_ch*8, m_ch*4*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2),\n",
    "            nn.Conv2d(m_ch*4, m_ch*2*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2),\n",
    "            nn.Conv2d(m_ch*2, m_ch*1*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), DepthToSpace(2),\n",
    "            nn.Conv2d(m_ch*1,        1, kernel_size=1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.image_decoder(z), self.mask_decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        first_item = pickle.load(f)\n",
    "        if isinstance(first_item, int):\n",
    "            return {list(w.keys())[0]: list(w.values())[0] for _ in range(first_item) for w in [pickle.load(f)]}\n",
    "    return np.load(model_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_encoder(tf_encoder_path):\n",
    "\n",
    "    weights = load_weights(tf_encoder_path)\n",
    "    encoder_dims = weights[\"down1/conv1/weight:0\"].shape[-1]\n",
    "    print(f\"encoder dims: {encoder_dims}\")\n",
    "\n",
    "    encoder = Encoder(encoder_dims)\n",
    "    encoder_state_dict = encoder.state_dict()\n",
    "\n",
    "    for i, ((layer_name, params), tf_params) in enumerate(zip(encoder_state_dict.items(), weights.values())):\n",
    "        if tf_params.ndim == 4:\n",
    "            tf_params = tf_params.transpose(3, 2, 0, 1)\n",
    "            \n",
    "        assert params.shape == tf_params.shape\n",
    "        # print(f\"Layer {i+1:02d}. torch_shape: {str(params.shape)[11:-1]:<16} | tf_shape: {tf_params.shape}\")\n",
    "        encoder_state_dict[layer_name] = torch.from_numpy(tf_params)\n",
    "\n",
    "    encoder.load_state_dict(encoder_state_dict)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def transfer_inter(tf_inter_path):\n",
    "\n",
    "    weights = load_weights(tf_inter_path)\n",
    "    in_features  = weights['dense1/weight:0'].shape[0]\n",
    "    ae_dims      = weights['dense1/weight:0'].shape[1]\n",
    "    out_features = weights['dense2/weight:0'].shape[1]\n",
    "    print(f\"inter dims: {in_features} -> {ae_dims} -> {out_features}\")\n",
    "\n",
    "    inter = nn.Sequential(nn.Linear(in_features, ae_dims), nn.Linear(ae_dims, out_features))\n",
    "    inter_state_dict = inter.state_dict()\n",
    "\n",
    "    for i, ((layer_name, params), tf_params) in enumerate(zip(inter_state_dict.items(), weights.values())):\n",
    "        if tf_params.ndim == 2:\n",
    "            tf_params = tf_params.transpose(1, 0)\n",
    "            \n",
    "        assert params.shape == tf_params.shape\n",
    "        # print(f\"Layer {i+1:02d}. torch_shape: {str(params.shape)[11:-1]:<16} | tf_shape: {tf_params.shape}\")\n",
    "        inter_state_dict[layer_name] = torch.from_numpy(tf_params)\n",
    "\n",
    "    inter.load_state_dict(inter_state_dict)\n",
    "    return inter\n",
    "\n",
    "\n",
    "def transfer_decoder(tf_decoder_path):\n",
    "\n",
    "    weights = load_weights(tf_decoder_path)\n",
    "\n",
    "    decoder = Decoder(2048, 64, 32)\n",
    "    decoder_state_dict = decoder.state_dict()\n",
    "\n",
    "    decoder_layer_order = [\n",
    "        \"upscale0/conv1/weight:0\", \"upscale0/conv1/bias:0\", \"res0/conv1/weight:0\", \"res0/conv1/bias:0\", \"res0/conv2/weight:0\", \"res0/conv2/bias:0\",\n",
    "        \"upscale1/conv1/weight:0\", \"upscale1/conv1/bias:0\", \"res1/conv1/weight:0\", \"res1/conv1/bias:0\", \"res1/conv2/weight:0\", \"res1/conv2/bias:0\",\n",
    "        \"upscale2/conv1/weight:0\", \"upscale2/conv1/bias:0\", \"res2/conv1/weight:0\", \"res2/conv1/bias:0\", \"res2/conv2/weight:0\", \"res2/conv2/bias:0\",\n",
    "        \"upscale3/conv1/weight:0\", \"upscale3/conv1/bias:0\", \"res3/conv1/weight:0\", \"res3/conv1/bias:0\", \"res3/conv2/weight:0\", \"res3/conv2/bias:0\",\n",
    "        \"out_conv/weight:0\", \"out_conv/bias:0\", \"out_conv1/weight:0\", \"out_conv1/bias:0\", \"out_conv2/weight:0\", \"out_conv2/bias:0\", \"out_conv3/weight:0\", \"out_conv3/bias:0\",\n",
    "        \"upscalem0/conv1/weight:0\", \"upscalem0/conv1/bias:0\", \"upscalem1/conv1/weight:0\", \"upscalem1/conv1/bias:0\", \"upscalem2/conv1/weight:0\", \"upscalem2/conv1/bias:0\",\n",
    "        \"upscalem3/conv1/weight:0\", \"upscalem3/conv1/bias:0\", \"upscalem4/conv1/weight:0\", \"upscalem4/conv1/bias:0\", \"out_convm/weight:0\", \"out_convm/bias:0\",\n",
    "    ]\n",
    "\n",
    "    for i, ((layer_name, params), tf_name) in enumerate(zip(decoder_state_dict.items(), decoder_layer_order)):\n",
    "        tf_params = weights[tf_name]\n",
    "\n",
    "        if tf_params.ndim == 4:\n",
    "            tf_params = tf_params.transpose(3, 2, 0, 1)\n",
    "            \n",
    "        assert params.shape == tf_params.shape\n",
    "        # print(f\"Layer {i+1:02d}. {layer_name:<36}: {str(params.shape)[11:-1]:<20} | {tf_name:<30}: {tf_params.shape}\")\n",
    "        decoder_state_dict[layer_name] = torch.from_numpy(tf_params)\n",
    "\n",
    "    decoder.load_state_dict(decoder_state_dict)\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class IDTransfer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IDTransfer, self).__init__()\n",
    "        self.encoder  = transfer_encoder(Path(r\"...\" ))\n",
    "        self.inter_AB = transfer_inter  (Path(r\"...\"))\n",
    "        self.decoder  = transfer_decoder(Path(r\"...\" ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inter_AB(self.encoder(x)).reshape(-1, 1024, 7, 7)\n",
    "        return self.decoder(torch.concat((x, x), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import numexpr as ne\n",
    "\n",
    "def reinhard_color_transfer(target : np.ndarray, source : np.ndarray, target_mask : np.ndarray = None, source_mask : np.ndarray = None, mask_cutoff=0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transfer color using rct method.\n",
    "\n",
    "        target      np.ndarray H W 3C   (BGR)   np.float32\n",
    "        source      np.ndarray H W 3C   (BGR)   np.float32\n",
    "\n",
    "        target_mask(None)   np.ndarray H W 1C  np.float32\n",
    "        source_mask(None)   np.ndarray H W 1C  np.float32\n",
    "        \n",
    "        mask_cutoff(0.5)    float\n",
    "\n",
    "    masks are used to limit the space where color statistics will be computed to adjust the target\n",
    "\n",
    "    reference: Color Transfer between Images https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf\n",
    "    \"\"\"\n",
    "    source = cv2.cvtColor(source, cv2.COLOR_BGR2LAB)\n",
    "    target = cv2.cvtColor(target, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    source_input = source\n",
    "    if source_mask is not None:\n",
    "        source_input = source_input.copy()\n",
    "        source_input[source_mask[...,0] < mask_cutoff] = [0,0,0]\n",
    "    \n",
    "    target_input = target\n",
    "    if target_mask is not None:\n",
    "        target_input = target_input.copy()\n",
    "        target_input[target_mask[...,0] < mask_cutoff] = [0,0,0]\n",
    "\n",
    "    target_l_mean, target_l_std, target_a_mean, target_a_std, target_b_mean, target_b_std, \\\n",
    "        = target_input[...,0].mean(), target_input[...,0].std(), target_input[...,1].mean(), target_input[...,1].std(), target_input[...,2].mean(), target_input[...,2].std()\n",
    "    \n",
    "    source_l_mean, source_l_std, source_a_mean, source_a_std, source_b_mean, source_b_std, \\\n",
    "        = source_input[...,0].mean(), source_input[...,0].std(), source_input[...,1].mean(), source_input[...,1].std(), source_input[...,2].mean(), source_input[...,2].std()\n",
    "    \n",
    "    # not as in the paper: scale by the standard deviations using reciprocal of paper proposed factor\n",
    "    target_l = target[...,0]\n",
    "    target_l = ne.evaluate('(target_l - target_l_mean) * source_l_std / target_l_std + source_l_mean')\n",
    "\n",
    "    target_a = target[...,1]\n",
    "    target_a = ne.evaluate('(target_a - target_a_mean) * source_a_std / target_a_std + source_a_mean')\n",
    "    \n",
    "    target_b = target[...,2]\n",
    "    target_b = ne.evaluate('(target_b - target_b_mean) * source_b_std / target_b_std + source_b_mean')\n",
    "\n",
    "    np.clip(target_l,    0, 100, out=target_l)\n",
    "    np.clip(target_a, -127, 127, out=target_a)\n",
    "    np.clip(target_b, -127, 127, out=target_b)\n",
    "\n",
    "    return cv2.cvtColor(np.stack([target_l,target_a,target_b], -1), cv2.COLOR_LAB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from face_parser import FaceParser\n",
    "from face_detector import FaceDetector\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE  = torch.bfloat16\n",
    "\n",
    "face_detector = FaceDetector(models_dir=\"./\")\n",
    "id_transfer = IDTransfer().to(device=DEVICE, dtype=DTYPE)\n",
    "parser = FaceParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFrame(frame, enhancer = None, detectiom_res: int = 512, model_res: int = 224, num_pixel_shifts: int = 2):\n",
    "\n",
    "    process_res = model_res * num_pixel_shifts\n",
    "    out_frame = frame\n",
    "    detected_face, M = face_detector.get_face(frame=frame, image_size=detectiom_res, zoom_out_factor=1)\n",
    "\n",
    "    if M is not None:\n",
    "        detected_face = cv2.resize(detected_face, (process_res, process_res))\n",
    "        input_face = torch.tensor(detected_face, device=DEVICE, dtype=DTYPE).permute(2, 0, 1)\n",
    "        input_face = input_face.reshape(3, model_res, num_pixel_shifts, model_res, num_pixel_shifts).permute(2,4,0,1,3)\n",
    "        input_face = input_face.reshape(num_pixel_shifts*num_pixel_shifts, 3, model_res, model_res) / 255.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_img, pred_msk = id_transfer(input_face)\n",
    "\n",
    "        pred_img = pred_img.view(num_pixel_shifts, num_pixel_shifts, 3, model_res, model_res).permute(2,3,0,4,1).reshape(3,process_res, process_res)\n",
    "        pred_msk = pred_msk.view(num_pixel_shifts, num_pixel_shifts, 1, model_res, model_res).permute(2,3,0,4,1).reshape(1,process_res, process_res)\n",
    "\n",
    "        pred_img = pred_img                .to(device=\"cpu\", dtype=torch.float32).permute(1, 2, 0).numpy()\n",
    "        pred_msk = pred_msk.repeat(3, 1, 1).to(device=\"cpu\", dtype=torch.float32).permute(1, 2, 0).numpy()\n",
    "\n",
    "        # pred_msk = cv2.dilate(pred_msk, np.ones((21,21), np.uint8), iterations=1)\n",
    "        pred_msk = cv2.GaussianBlur(pred_msk, (11, 11), 0)\n",
    "\n",
    "        pred_img = reinhard_color_transfer(pred_img, (detected_face/255.).astype(np.float32), target_mask=pred_msk, source_mask=pred_msk)\n",
    "        pred_img = pred_img * 255.\n",
    "\n",
    "        if enhancer:\n",
    "            pred_img = enhancer.enhance(pred_img, blend=0.4)\n",
    "\n",
    "        pred_img = cv2.resize(pred_img, (detectiom_res, detectiom_res))\n",
    "        pred_msk = cv2.resize(pred_msk, (detectiom_res, detectiom_res))\n",
    "        out_mask  = cv2.warpAffine(pred_msk, cv2.invertAffineTransform(M), dsize=frame.shape[1::-1], borderValue=(0,0,0))\n",
    "        out_frame = cv2.warpAffine(pred_img, cv2.invertAffineTransform(M), dsize=frame.shape[1::-1])\n",
    "        out_frame = (out_mask*out_frame + (1-out_mask)*frame).astype(np.uint8)\n",
    "\n",
    "    return out_frame\n",
    "\n",
    "\n",
    "def process(input_video: Path, output_path: Path, enhancer, detectiom_res: int = 512, model_res: int = 224, num_pixel_shifts: int = 2):\n",
    "\n",
    "    input_stream  = cv2.VideoCapture(filename=str(input_video))\n",
    "    output_stream = cv2.VideoWriter(\n",
    "        filename  = str(output_path), \n",
    "        fourcc    = cv2.VideoWriter_fourcc(*\"mp4v\"), \n",
    "        fps       = int(input_stream.get(cv2.CAP_PROP_FPS)),\n",
    "        frameSize = (int(input_stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(input_stream.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        )\n",
    "\n",
    "    total_frames = int(input_stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = input_stream.read()\n",
    "        if not ret: break\n",
    "        frame = processFrame(frame, enhancer, detectiom_res, model_res, num_pixel_shifts)\n",
    "        output_stream.write(frame)\n",
    "    \n",
    "    output_stream = None\n",
    "    input_stream.release()\n",
    "\n",
    "    subprocess.run(f\"ffmpeg -i {str(output_path)} -i {str(input_video)} -shortest -c copy out.mp4 -y\")\n",
    "    os.remove(str(output_path))\n",
    "    Path(\"out.mp4\").rename(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in Path(r\"...\").iterdir():\n",
    "    process(\n",
    "        input_video=video,\n",
    "        output_path=Path(\"...\")/video.name,\n",
    "        enhancer=None,\n",
    "        detectiom_res=512,\n",
    "        model_res=224,\n",
    "        num_pixel_shifts=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
