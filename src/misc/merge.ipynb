{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + 1e-8)\n",
    "    \n",
    "class Res(nn.Module):\n",
    "    def __init__(self, n_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(n_ch, n_ch, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(n_ch, n_ch, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.fuse = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fuse(x + self.conv(x))\n",
    "    \n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, n_ch):\n",
    "        super().__init__()\n",
    "        self.out_conv = nn.ModuleList([\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=1),\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(n_ch, 3, kernel_size=3, padding=1),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([i(x) for i in self.out_conv], dim=1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_ch):\n",
    "        super().__init__()\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(     3, n_ch*1, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True), Res(n_ch*1),\n",
    "            nn.Conv2d(n_ch*1, n_ch*2, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(n_ch*2, n_ch*4, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(n_ch*4, n_ch*8, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(n_ch*8, n_ch*8, kernel_size=5, stride=2, padding=2), nn.LeakyReLU(0.1, inplace=True), Res(n_ch*8),\n",
    "            nn.Flatten(), PixelNorm()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.image_encoder(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, i_ch, m_ch):\n",
    "        super().__init__()\n",
    "        self.image_decoder = nn.Sequential(\n",
    "            nn.Conv2d(   512, i_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2), Res(i_ch*8),\n",
    "            nn.Conv2d(i_ch*8, i_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2), Res(i_ch*8),\n",
    "            nn.Conv2d(i_ch*8, i_ch*4*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2), Res(i_ch*4),\n",
    "            nn.Conv2d(i_ch*4, i_ch*2*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2), Res(i_ch*2),\n",
    "            OutConv(i_ch*2), nn.PixelShuffle(2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.mask_decoder = nn.Sequential(\n",
    "            nn.Conv2d(   512, m_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2),\n",
    "            nn.Conv2d(m_ch*8, m_ch*8*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2),\n",
    "            nn.Conv2d(m_ch*8, m_ch*4*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2),\n",
    "            nn.Conv2d(m_ch*4, m_ch*2*4, kernel_size=3, padding=1), nn.LeakyReLU(0.1, inplace=True), nn.PixelShuffle(2),\n",
    "            nn.Conv2d(m_ch*2, 1*4, kernel_size=1), nn.PixelShuffle(2), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.image_decoder(z), self.mask_decoder(z)\n",
    "\n",
    "\n",
    "dtype  = torch.float32\n",
    "device = torch.device('cuda:0')\n",
    "image_size   = 512\n",
    "process_size = 128\n",
    "\n",
    "encoder = torch.load(r\"...\").to(device=device, dtype=dtype)\n",
    "decoder = torch.load(r\"...\").to(device=device, dtype=dtype)\n",
    "mlp     = torch.load(r\"...\").to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from face_parser import FaceParser\n",
    "from face_detector import FaceDetector\n",
    "\n",
    "parser = FaceParser()\n",
    "face_detector = FaceDetector(models_dir=\"./\")\n",
    "\n",
    "\n",
    "def process(input_video: Path, output_path: Path):\n",
    "\n",
    "    input_stream  = cv2.VideoCapture(filename=str(input_video))\n",
    "    output_stream = cv2.VideoWriter(\n",
    "        filename  = str(output_path), \n",
    "        fourcc    = cv2.VideoWriter_fourcc(*\"mp4v\"), \n",
    "        fps       = int(input_stream.get(cv2.CAP_PROP_FPS)),\n",
    "        frameSize = (int(input_stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(input_stream.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        )\n",
    "\n",
    "    scale = image_size // process_size\n",
    "    total_frames = int(input_stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = input_stream.read()\n",
    "        if not ret: break\n",
    "\n",
    "        out_frame  = frame\n",
    "        input_face, M = face_detector.get_face(frame=frame, image_size=image_size, zoom_out_factor=1)\n",
    "\n",
    "        if M is not None:\n",
    "            input_face = cv2.cvtColor(input_face, cv2.COLOR_BGR2RGB)\n",
    "            input_face = torch.tensor(input_face, device=device, dtype=dtype).permute(2, 0, 1)\n",
    "            input_face = input_face.reshape(3, process_size, scale, process_size, scale).permute(2,4,0,1,3)\n",
    "            input_face = input_face.reshape(scale*scale,3,process_size,process_size).to(torch.float32) / 255.\n",
    "\n",
    "            with torch.no_grad():\n",
    "                codes = mlp(encoder(input_face)).view(-1, 256, process_size//32, process_size//32)\n",
    "                pred_img, pred_msk = decoder(torch.concat([codes, codes], dim=1))\n",
    "\n",
    "            pred_img = pred_img.view(scale,scale,3,process_size,process_size).permute(2,3,0,4,1).reshape(3,image_size,image_size)          \n",
    "            pred_msk = pred_msk.view(scale,scale,1,process_size,process_size).permute(2,3,0,4,1).reshape(1,image_size,image_size)          \n",
    "\n",
    "            pred_img = (pred_img.clamp(0,1)*255).to(device=\"cpu\", dtype=torch.uint8).permute(1, 2, 0).numpy()\n",
    "            pred_msk = (pred_msk.repeat(3, 1, 1).clamp(0,1)).to(device=\"cpu\").permute(1, 2, 0).numpy()\n",
    "\n",
    "            output_face = cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            out_mask  = cv2.warpAffine(pred_msk   , cv2.invertAffineTransform(M), dsize=frame.shape[1::-1], borderValue=(0,0,0))\n",
    "            out_frame = cv2.warpAffine(output_face, cv2.invertAffineTransform(M), dsize=frame.shape[1::-1])\n",
    "            out_frame = (out_mask*out_frame + (1-out_mask)*frame).astype(np.uint8)\n",
    "\n",
    "        output_stream.write(out_frame)\n",
    "    \n",
    "    output_stream.release()\n",
    "    input_stream.release()\n",
    "\n",
    "    subprocess.run(f\"ffmpeg -i {str(output_path)} -i {str(input_video)} -shortest -c copy out.mp4 -y\")\n",
    "    output_path.unlink()\n",
    "    Path(\"out.mp4\").rename(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in list(Path(r\"...\").iterdir()):\n",
    "    process(\n",
    "        input_video=video,\n",
    "        output_path=Path(\"outs\")/video.name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
